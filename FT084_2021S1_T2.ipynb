{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FT084_2021S1_T2.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wn-Wx6fkmffh"
      ],
      "authorship_tag": "ABX9TyP5WVV2kb1jgXVn537DRcX0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SamuelOliveira/Colab/blob/main/FT084_2021S1_T2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EEleGszyivOi"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://moodle.ggte.unicamp.br/pluginfile.php/1/core_admin/logocompact/300x300/1615465513/unicamp_transp_20180409.png\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "# Tarefa 2: Árvores de Decisão, Naïve Bayes, k-NN e Ensembles\n",
        "\n",
        "\n",
        "**Enunciado:**\n",
        "\n",
        "Dados os pacotes de software para classificação de dados identificados na tarefa anterior, pede-se:\n",
        "\n",
        "* Escolha uma destas ferramentas, que tenha implementações de algoritmos de árvores de decisão, naïve Bayes e k-NN, e aplique estes **três algoritmos** ao conjunto de dados [Liver Disorder](http://goo.gl/QCuYTc):\n",
        "  - Utilize subamostragem aleatória com 5 repetições para cada algoritmo e apresente o erro de classificação **médio** de cada um (para os conjuntos de testes);\n",
        "  - Adote uma divisão de 70% dos dados para treinamento e 30% dos dados para teste;\n",
        "  - Faça a amostragem antes de iniciar o treinamento e use os mesmos dados para todos os algoritmos (em cada repetição);\n",
        "* Para cada repetição, monte um **ensemble** com os classificadores já treinados (via voto majoritário), aplique ao conjunto de testes e apresente o desempenho médio.\n",
        "\n",
        "**ATENÇÃO**: Não se esqueça de apresentar no relatório os **parâmetros** definidos para cada algoritmo (caso existam)!\n",
        "\n",
        "**Formato:**\n",
        "\n",
        "*   Fonte: Times New Roman ou Calibri;\n",
        "*   Tamanho da fonte: 12pt;\n",
        "*   Espaçamento: 1,15;\n",
        "*   Margens: 2cm;\n",
        "*   **ATENÇÃO**: seja sucinto e completo.\n",
        "\n",
        "**Informações Adicionais:**\n",
        "\n",
        "Entregar o texto em PDF e o **arquivo de dados** no formato da ferramenta escolhida (em um único arquivo .ZIP)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn-Wx6fkmffh"
      },
      "source": [
        "# Importando Bilbiotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t148jzIUmjDn"
      },
      "source": [
        "import itertools\n",
        "import pydotplus\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.externals.six import StringIO\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, log_loss\n",
        "\n",
        "# DECLARANDO FUNÇÃO PARA PLOTAR A MATRIZ DE CONFUSÃO\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='',\n",
        "                          cmap=plt.cm.Blues):\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title('Matriz de Confusão ' + title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    \n",
        "    # if normalize:\n",
        "    #     cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    #     print(\"Matriz de Confusão Normalizada\")\n",
        "    # else:\n",
        "    #     print('Matriz de Confusão Desnormalizada')\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, cm[i, j],\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Rótulo Real')\n",
        "    plt.xlabel('Rótulo Previsto')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsls_aoFm5KP"
      },
      "source": [
        "# Pre-processamento\n",
        "\n",
        "* Criando o DataFrame\n",
        "* Tratando Dados Faltantes\n",
        "  - Se Houver Dados Faltantes Remove (para evitar ruídos e distorções)\n",
        "* Discretizando Dados Continuos\n",
        "  - atributo **drinks**\n",
        "* Preparando Subamostragem\n",
        "  - 70% Treino\n",
        "  - 30% Teste"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIrLXZPqm9rS"
      },
      "source": [
        "df = pd.read_csv('sample_data/bupa.csv')\n",
        "\n",
        "# Removendo dados faltantes\n",
        "df = df.dropna()\n",
        "\n",
        "# PEGANDO OS NUMEROS DE ATRIBUTOS\n",
        "count_col = df.shape[1]\n",
        "\n",
        "# PEGANDO OS NÚMEROS DE REGISTROS\n",
        "count_row = df.shape[0]\n",
        "\n",
        "name_label = 'drinks_label'\n",
        "name_features = ['mcv','alkphos','sgpt','sgot','gammagt']\n",
        "name_categoric = ['abstêmio', 'socialmente', 'alcoólatra']\n",
        "\n",
        "# Discretizando o DataFrame\n",
        "bins = [-1, 2, 5, 20]\n",
        "df[name_label] = pd.cut(df['drinks'].to_numpy(), bins=bins, labels=name_categoric)\n",
        "\n",
        "balanced_a = df[df[name_label] == name_categoric[0]].shape[0] * 100 / count_row\n",
        "balanced_b = df[df[name_label] == name_categoric[1]].shape[0] * 100 / count_row\n",
        "balanced_c = df[df[name_label] == name_categoric[2]].shape[0] * 100 / count_row\n",
        "\n",
        "balanced_a_f = \"{1:.2f}%\".format(\"\", balanced_a)\n",
        "balanced_b_f = \"{1:.2f}%\".format(\"\", balanced_b)\n",
        "balanced_c_f = \"{1:.2f}%\".format(\"\", balanced_c)\n",
        "\n",
        "# Verificando Balanceamento do DataSet\n",
        "print(\"%s   : %s %d amostras\" % (name_categoric[0], balanced_a_f, df[df[name_label] == name_categoric[0]].shape[0]))\n",
        "print(\"%s: %s %d amostras\" % (name_categoric[1], balanced_b_f, df[df[name_label] == name_categoric[1]].shape[0]))\n",
        "print(\"%s : %s %d amostras\" % (name_categoric[2], balanced_c_f, df[df[name_label] == name_categoric[2]].shape[0]))\n",
        "\n",
        "# PREPARANDO O SPLIT DO DATAFRAME 70/30, AMOSTRA ALEATORIA\n",
        "df_tr = df.sample(frac=0.70)\n",
        "df_ts = df.sample(frac=0.30)\n",
        "\n",
        "Y_train, Y_test = df_tr[name_label].values, df_ts[name_label].values\n",
        "X_train, X_test = df_tr[name_features].values, df_ts[name_features].values\n",
        "\n",
        "# NORMALIZANDO AMOSTRAS DE TREINO\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "X_train = scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# ARMAZENANDO AS AMOSTRAS\n",
        "df_tr.to_csv('sample_data/bupa_train.csv')\n",
        "df_ts.to_csv('sample_data/bupa_test.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_loqecpP6wM"
      },
      "source": [
        "# Criando e Treinando os Modelos\n",
        "\n",
        "*   Decision Tree;\n",
        "  - Cinco níveis para evitar overfiting\n",
        "*   Naïve Bayes;\n",
        "*   k-NN.\n",
        "  - Treinando menor taxa de erros k=? ( vizinho mais próximo )"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ga1UbxTVP9zi"
      },
      "source": [
        "decisionTree = DecisionTreeClassifier(criterion='entropy', max_depth=5)\n",
        "model_tree = decisionTree.fit(X_train, Y_train)\n",
        "\n",
        "neighbors = 10\n",
        "\n",
        "error = []\n",
        "for i in range(1, neighbors):\n",
        "    knn = KNeighborsClassifier(n_neighbors=i)\n",
        "    knn.fit(X_train, Y_train)\n",
        "    pred_i = knn.predict(X_test)\n",
        "    error.append(np.mean(pred_i != Y_test))\n",
        "\n",
        "naive_bayes = GaussianNB()\n",
        "model_naive = naive_bayes.fit(X_train, Y_train)\n",
        "\n",
        "kNeighbors = KNeighborsClassifier(n_neighbors=neighbors)\n",
        "model_k_nn = kNeighbors.fit(X_train, Y_train)\n",
        "\n",
        "name_classes = model_tree.classes_\n",
        "\n",
        "# Plotando Taxa de Erros K\n",
        "plt.figure(figsize=(9, 4))\n",
        "plt.plot(range(1, neighbors), error, color='pink', linestyle='dashed', marker='o',markerfacecolor='yellow', markersize=15)\n",
        "plt.title('Error Rate K Value')\n",
        "plt.xlabel('K Value')\n",
        "plt.ylabel('Mean Error')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BavTS3vBmoC7"
      },
      "source": [
        "# Plotando a Decision Tree\n",
        "\n",
        "Plotando a Decision Tree de 6 niveis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLwVSw6nmpUd"
      },
      "source": [
        "# Plotar a Decision Tree\n",
        "dot_data = StringIO()\n",
        "export_graphviz(model_tree, out_file=dot_data, filled=True, feature_names=name_features, class_names=name_classes, rounded=True, special_characters=True)\n",
        "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
        "Image(graph.create_png())\n",
        "graph.write_png(\"model_tree.png\")\n",
        "Image('model_tree.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zNMN74XQs--"
      },
      "source": [
        "# Atributos mais importantes\n",
        "\n",
        "Features de maior importância para o modelo treinado"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y00hysmoQwE2"
      },
      "source": [
        "# ARMAZENANDO AS ETIQUETAS EM Y, CLASSES\n",
        "Y = df[name_label].values\n",
        "\n",
        "# ARMAZENANDO OS VALORES EM X, FEATURES/ATRIBUTOS\n",
        "X = df[name_features].values\n",
        "\n",
        "importances = model_tree.feature_importances_\n",
        "indices = np.argsort(importances)[::-1]\n",
        "print(\"Rank dos atributos:\")\n",
        "print('\\n')\n",
        "\n",
        "for f in range(X.shape[1]):\n",
        "    print(\"%d. feature %d (%f) %s\" % (f + 1, indices[f], importances[indices[f]], name_features[indices[f]]))\n",
        "print('\\n')\n",
        "\n",
        "f, ax = plt.subplots(figsize=(9, 4))\n",
        "plt.title(\"Feature ranking\", fontsize = 20)\n",
        "plt.bar(range(X.shape[1]), importances[indices], color=\"b\", align=\"center\")\n",
        "plt.xticks(range(X.shape[1]), indices)\n",
        "plt.xlim([-1, X.shape[1]])\n",
        "plt.ylabel(\"importance\", fontsize = 18)\n",
        "plt.xlabel(\"index of the feature\", fontsize = 18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buA9Hol6Q12G"
      },
      "source": [
        "# Testando Modelo(Decision Tree)\n",
        "\n",
        "* Predições;\n",
        "* Matriz de Confusão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fysmhSa2Q5Mw"
      },
      "source": [
        "Y_pred_tree = model_tree.predict(X_test)\n",
        "\n",
        "print(\"ACURÁCIA TREE: \", \"{1:.2f}%\".format(\"\", 100 * accuracy_score(Y_test, Y_pred_tree)))\n",
        "print('\\n')\n",
        "print(classification_report(Y_test, Y_pred_tree))\n",
        "\n",
        "# precision:  DAS CLASSIFICAÇÕES QUE O MODELO FEZ PARA CADA CLASSE\n",
        "# recall:     DOS POSSÍVEIS DATAPOINTS PERTECENTES A CADA CLASSE\n",
        "\n",
        "matrix_confusao = confusion_matrix(Y_test, Y_pred_tree)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(matrix_confusao, classes=name_classes, title='Decision Tree')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE_et0NBrr6L"
      },
      "source": [
        "# Testando Modelo(Naïve Bayes)\n",
        "* Predições;\n",
        "* Matriz de Confusão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVrtFXtlrv-y"
      },
      "source": [
        "Y_pred_naive = model_naive.predict(X_test)\n",
        "\n",
        "print(\"ACURÁCIA NAIVE: \", \"{1:.2f}%\".format(\"\", 100 * accuracy_score(Y_test, Y_pred_naive)))\n",
        "print('\\n')\n",
        "print(classification_report(Y_test, Y_pred_naive))\n",
        "\n",
        "# PRECISÃO: DAS CLASSIFICAÇÕES QUE O MODELO FEZ PARA UMA DETERMINADA CLASSE\n",
        "# RECALL: DOS POSSÍVEIS DATAPOINTS PERTECENTES A UMA DETERMINADA CLASSE\n",
        "\n",
        "matrix_confusao = confusion_matrix(Y_test, Y_pred_naive)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(matrix_confusao, classes=name_classes,title='Naïve Bayes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZGsMqfer1ht"
      },
      "source": [
        "# Testando Modelo(k-NN)\n",
        "* Predições;\n",
        "* Matriz de Confusão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53K3lCRQr2Wu"
      },
      "source": [
        "Y_pred_k_nn = model_k_nn.predict(X_test)\n",
        "\n",
        "print(\"ACURÁCIA k-NN: \", \"{1:.2f}%\".format(\"\", 100 * accuracy_score(Y_test, Y_pred_k_nn)))\n",
        "print('\\n')\n",
        "print(classification_report(Y_test, Y_pred_k_nn))\n",
        "\n",
        "# PRECISÃO: DAS CLASSIFICAÇÕES QUE O MODELO FEZ PARA UMA DETERMINADA CLASSE\n",
        "# RECALL: DOS POSSÍVEIS DATAPOINTS PERTECENTES A UMA DETERMINADA CLASSE\n",
        "\n",
        "matrix_confusao = confusion_matrix(Y_test, Y_pred_k_nn)\n",
        "plt.figure()\n",
        "plot_confusion_matrix(matrix_confusao, classes=name_classes,title='k-NN')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cE95Ay6usBq4"
      },
      "source": [
        "# Ensemble"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_ed3ynEsEHA"
      },
      "source": [
        "# # https://stackabuse.com/ensemble-voting-classification-in-python-with-scikit-learn/\n",
        "# # https://www.kaggle.com/alexisbercion/majority-voting-ensemble\n",
        "\n",
        "voting_clf = VotingClassifier(estimators=[('m_tree', decisionTree), ('m_naive', naive_bayes), ('m_k_nn', kNeighbors)], voting='soft')\n",
        "voting_clf.fit(X_train, Y_train)\n",
        "preds = voting_clf.predict(X_test)\n",
        "\n",
        "print(\"ACURÁCIA Ensemble: \", \"{1:.2f}%\".format(\"\", 100 * accuracy_score(Y_test, preds)))\n",
        "\n",
        "\n",
        "# https://scikit-learn.org/stable/auto_examples/ensemble/plot_voting_probas.html#sphx-glr-auto-examples-ensemble-plot-voting-probas-py"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}